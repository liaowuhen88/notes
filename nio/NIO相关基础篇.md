## 用户空间以及内核空间概念

我们知道现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。

操心系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。

为了保证用户进程不能直接操作内核，保证内核的安全，操心系统将虚拟空间划分为两部分，

一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），

供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。

每个进程可以通过系统调用进入内核，因此，Linux内核由系统内的所有进程共享。于是，从具体进程的角度来看，

每个进程可以拥有4G字节的虚拟空间。空间分配如下图所示：

![Image text](img/1589179846.jpg)

有了用户空间和内核空间，整个linux内部结构可以分为三部分，从最底层到最上层依次是：硬件-->内核空间-->用户空间。

![Image text](img/1589179938.jpg)

需要注意的细节问题，从上图可以看出内核的组成:

* 1.内核空间中存放的是内核代码和数据，而进程的用户空间中存放的是用户程序的代码和数据。不管是内核空间还是用户空间，它们都处于虚拟空间中。

* 2.Linux使用两级保护机制：0级供内核使用，3级供用户程序使用。

**理论上：32位=2^32B = 4 * 2^30B = 4GB，这是 32 位下单进程内存上限**

**目前（2015年5月），Intel的32位架构下，可使用的地址线是36个，可使用的最大物理地址是2^36B，折合64GB，可用的地址空间是4GB。**

**64位架构下，地址线是46个，所以最大的物理地址是2^46B，折合64TB，可用地址空间也是这么大（目前为止）**

## Linux 网络 I/O模型

我们都知道，为了OS的安全性等的考虑，进程是无法直接操作I/O设备的，其必须通过系统调用请求内核来协助完成I/O动作，

而内核会为每个I/O设备维护一个buffer。

如下图所示：

![Image text](img/1589181887.jpg)

整个请求过程为： 用户进程发起请求，内核接受到请求后，从I/O设备中获取数据到buffer中，再将buffer中的数据copy到用户进程的地址空间，

该用户进程获取到数据后再响应客户端。在整个请求过程中，数据输入至buffer需要时间，而从buffer复制数据至进程也需要时间。

因此根据在这两段时间内等待方式的不同，I/O动作可以分为以下五种模式：

* 阻塞I/O (Blocking I/O)

* 非阻塞I/O (Non-Blocking I/O)

* I/O复用（I/O Multiplexing)

* 信号驱动的I/O (Signal Driven I/O)

* 异步I/O (Asynchrnous I/O) 

### 记住这两点很重要

* 1 等待数据准备 (Waiting for the data to be ready)
* 2 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)

### 阻塞I/O (Blocking I/O)

在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：

![Image text](img/1589182066.jpg)

当用户进程调用了recvfrom这个系统调用，

* 内核就开始了IO的第一个阶段：等待数据准备。对于network io来说，

  很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候内核就要等待足够的数据到来。

  而在用户进程这边，整个进程会被阻塞。

* 当内核一直等到数据准备好了，它就会将数据从内核中拷贝到用户内存，

  然后内核返回结果，用户进程才解除block的状态，重新运行起来。

**所以，blocking IO的特点就是在IO执行的两个阶段都被block了。**

### 非阻塞I/O (Non-Blocking I/O)

linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：

![Image text](img/1589182310.jpg)

当用户进程调用recvfrom时，系统不会阻塞用户进程，而是立刻返回一个ewouldblock错误，从用户进程角度讲 ，并不需要等待，

而是马上就得到了一个结果。用户进程判断标志是ewouldblock时，就知道数据还没准备好，于是它就可以去做其他的事了，

于是它可以再次发送recvfrom，一旦内核中的数据准备好了。并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。

当一个应用程序在一个循环里对一个非阻塞调用recvfrom，我们称为轮询。应用程序不断轮询内核，看看是否已经准备好了某些操作。

这通常是浪费CPU时间，但这种模式偶尔会遇到。

### I/O复用（I/O Multiplexing)

IO multiplexing这个词可能有点陌生，但是如果我说select，epoll，大概就都能明白了。有些地方也称这种IO方式为event driven IO。我们都知道，

select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个function会不断的轮询所负责的所有socket，

当某个socket有数据到达了，就通知用户进程。它的流程如图：

![Image text](img/1589186740.jpg)

* 当用户进程调用了select，那么整个进程会被block，而同时，内核会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，
  select就会返回。
* 这个时候用户进程再调用read操作，将数据从内核拷贝到用户进程。

这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，

而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。

（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO

 的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，

但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。

### 文件描述符fd

Linux的内核将所有外部设备都可以看做一个文件来操作。那么我们对与外部设备的操作都可以看做对文件进行操作。

我们对一个文件的读写，都通过调用内核提供的系统调用；内核给我们返回一个filede scriptor（fd,文件描述符）。

而对一个socket的读写也会有相应的描述符，称为socketfd(socket描述符）。描述符就是一个数字，

指向内核中一个结构体（文件路径，数据区，等一些属性）。那么我们的应用程序对文件的读写就通过对描述符的读写完成。

### select

